# Loading and parsing the 5000 points data
# Clustering is the unsupervised learning task that involves grouping objects into clusters of high similarity.
# Unlike the supervised tasks, where data is labeled, clustering can be used to make sense of unlabeled data.
# PySpark MLlib includes the popular K-means algorithm for clustering. 
# In this 3 part exercise, you'll find out how many clusters are there in a dataset containing 5000 rows and 2 columns. 
# For this you'll first load the data into an RDD, parse the RDD based on the delimiter, run the KMeans model, evaluate the model and finally visualize the clusters.

# K-means training
# Now that the RDD is ready for training, in this 2nd part, you'll test it with k's from 13 to 16 (to save computation time) and use the elbow method to chose the correct k.
# The idea of the elbow method is to run K-means clustering on the dataset for different values of k, calculate Within Set Sum of Squared Error (WSSSE),
# and select the best k based on the sudden drop in WSSSE, i.e. where the elbow occurs. 

# Visualizing clusters
# You just trained the k-means model with an optimum k value (k=16) and generated cluster centers (centroids). 
# you will visualize the clusters and the centroids by overlaying them.
# This will indicate how well the clustering worked (ideally, the clusters should be distinct from each other and centroids should be at the center of their respective clusters).
# To achieve this, you will first convert the rdd_split_int RDD into a Spark DataFrame, and then into Pandas DataFrame which can be used for plotting. 
# Similarly, you will convert cluster_centers into a Pandas DataFrame. 
# Once both the DataFrames are created, you will create scatter plots using Matplotlib.
